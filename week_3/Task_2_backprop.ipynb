{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Task 2\n",
    "Implement basic backward pass using only numpy:\n",
    " - for your last week's Single Layer Perceptron.\n",
    " - for all activation functions\n",
    " - loss functions\n",
    "\n",
    "Perform forward pass and backward pass, and use the gradient check function to verify your implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import Module"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Linear Layer\n",
    "\n",
    "In your previous task you defined `forward(input)` pass for your Linear class. Now we continue in creation of your own framework a little further with defining the `backward(dNet)` function. In this little framework is activation and linear unit separated. This separation is benefit in backward propagation and optimization. (If you want to know why, take a look on implementation of forward and backward propagation in class Model.)\n",
    "\n",
    "Note, that now you are implementing backward pass for on only one sample, the lecture's framework was prepared for training on the whole dataset of $m$ samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "#   Linear class\n",
    "#------------------------------------------------------------------------------\n",
    "class Linear(Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(Linear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.W = np.random.randn(out_features, in_features)\n",
    "        self.dW = np.zeros_like(self.W)\n",
    "        self.b = np.zeros((out_features, 1)) # Watch-out for the shape\n",
    "        self.db = np.zeros_like(self.b)\n",
    "\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        self.fw_inputs = input\n",
    "        self.m = self.fw_inputs.shape[1]\n",
    "        net = np.matmul(self.W, input) + self.b\n",
    "        return net\n",
    "\n",
    "    def backward(self, dz: np.ndarray) -> np.ndarray:\n",
    "        # >>>>>>>>> add here\n",
    "        pass\n",
    "        # <<<<<<<<<"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Activations\n",
    "Implement backward pass for Sigmoid, Tanh and ReLU activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "#   SigmoidActivationFunction class\n",
    "#------------------------------------------------------------------------------\n",
    "class Sigmoid(Module):\n",
    "    def __init__(self):\n",
    "        super(Sigmoid, self).__init__()\n",
    "\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        self.fw_input = input\n",
    "        return 1.0 / (1.0 + np.exp(-input))\n",
    "\n",
    "    def backward(self, da) -> np.ndarray:\n",
    "        # >>>>>>>>> add here\n",
    "        pass\n",
    "        # <<<<<<<<<\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#   HyperbolicTangentActivationFunction class\n",
    "#------------------------------------------------------------------------------\n",
    "class Tanh(Module):\n",
    "    def __init__(self):\n",
    "        super(Tanh, self).__init__()\n",
    "\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        self.fw_input = input\n",
    "        return (np.exp(2 * input) - 1) / (np.exp(2 * input) + 1)\n",
    "\n",
    "    def backward(self, da) -> np.ndarray:\n",
    "        # >>>>>>>>> add here\n",
    "        pass\n",
    "        # <<<<<<<<<\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#   RELUActivationFunction class\n",
    "#------------------------------------------------------------------------------\n",
    "class ReLU(Module):\n",
    "    def __init__(self):\n",
    "        super(ReLU, self).__init__()\n",
    "\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        self.fw_input = input\n",
    "        return np.maximum(input, 0)\n",
    "\n",
    "    def backward(self, da) -> np.ndarray:\n",
    "        # >>>>>>>>> add here\n",
    "        pass\n",
    "        # <<<<<<<<<"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Loss functions\n",
    "For successful backward pass, the computation and derivation of Loss function is necessary.\n",
    "The most common Loss functions are **Mean Square Error** _(MSE, L2)_ **Mean Absolute Error** _(MAE, L1)_ and **Binary Cross Entropy** _(BCE, Log Loss)_ and their modifications according to what is better for the current dataset.\n",
    "\n",
    "Implement MSE and BCE Loss functions as Modules of our little framework.\n",
    "\n",
    "Remember the difference between Loss and Cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "#   MeanSquareErrorLossFunction class\n",
    "#------------------------------------------------------------------------------\n",
    "class MSELoss(Module):\n",
    "    def __init__(self):\n",
    "        super(MSELoss, self).__init__()\n",
    "\n",
    "    def forward(self, input: np.ndarray, target: np.ndarray) -> np.ndarray:\n",
    "        # >>>>>>>>> add here\n",
    "        pass\n",
    "        # <<<<<<<<<\n",
    "\n",
    "    def backward(self, input: np.ndarray, target: np.ndarray) -> np.ndarray:\n",
    "        # >>>>>>>>> add here\n",
    "        pass\n",
    "        # <<<<<<<<<\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#   BinaryCrossEntropyLossFunction class\n",
    "#------------------------------------------------------------------------------\n",
    "class BCELoss(Module):\n",
    "    def __init__(self):\n",
    "        super(BCELoss, self).__init__()\n",
    "\n",
    "    def forward(self, input: np.ndarray, target: np.ndarray) -> np.ndarray:\n",
    "        # >>>>>>>>> add here\n",
    "        pass\n",
    "        # <<<<<<<<<\n",
    "\n",
    "    def backward(self, input: np.ndarray, target: np.ndarray) -> np.ndarray:\n",
    "        # >>>>>>>>> add here\n",
    "        pass\n",
    "        # <<<<<<<<<"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Model\n",
    "As in previous task, use `Model` class to encapsulate all layers of your MLP and define backward pass.\n",
    "Iterate over its modules stored in parameter OrderedDict `modules` -> `self.modules` in the correct order.\n",
    "\n",
    "Use call `.add_module(...)` to add layers of your MLP (network). Define MLP that could classify data from Circles dataset `dataset_Circles(...)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "#   Model class\n",
    "#------------------------------------------------------------------------------\n",
    "class Model(Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "    def forward(self, input) -> np.ndarray:\n",
    "        for name, module in self.modules.items():\n",
    "            # print(f'Layer fw:{name}, a.shape = {input.shape} \\n{input}')\n",
    "            input = module(input)\n",
    "            # print(f'z.shape = {input.shape} \\n{input}')\n",
    "        return input\n",
    "\n",
    "    def backward(self, dz: np.ndarray):\n",
    "        # >>>>>>>>> add here\n",
    "        pass\n",
    "        # <<<<<<<<<"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Main Processing Cell\n",
    "\n",
    " 1. Initialize dataset (`dataset_Circles`). [x]\n",
    " 2. Declare a simple model (at least 3 hidden layer). [x]\n",
    " 3. Perform forward pass through the network. [x]\n",
    " 4. Compute loss. []\n",
    " 5. Backward prop loss. []\n",
    " 6. Backward pass MLP. []\n",
    " 7. Check your computation of gradients via [`gradient_check`](https://datascience-enthusiast.com/DL/Improving_DeepNeural_Networks_Gradient_Checking.html) []\n",
    " 8. Start crying. []\n",
    " 9. Repeat until correct ;) []\n",
    " 10. ... (if error founds -> blame lecturer) []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from dataset import dataset_Circles\n",
    "from utils import gradient_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset_features_X, dataset_labels_Y = dataset_Circles(m=128, radius=0.7, noise=0.0)\n",
    "\n",
    "mlp = Model()\n",
    "mlp.add_module(Linear(2, 3), 'Dense_1')\n",
    "mlp.add_module(Tanh(), 'Tanh_1')\n",
    "mlp.add_module(Linear(3, 4), 'Dense_2')\n",
    "mlp.add_module(Tanh(), 'Tanh_2')\n",
    "mlp.add_module(Linear(4, 5), 'Dense_3')\n",
    "mlp.add_module(Tanh(), 'Tanh_3')\n",
    "mlp.add_module(Linear(5, 1), 'Dense_4_out')\n",
    "mlp.add_module(Sigmoid(), 'Sigmoid')\n",
    "\n",
    "predicted_Y_hat = mlp.forward(dataset_features_X) # Be careful with the shape - Loss vs Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "###>>> start of solution\n",
    "\n",
    "# TODO add loss function!!!\n",
    "loss_fn = None\n",
    "\n",
    "###<<< end of solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Verify your solution!\n",
    "gradient_check(mlp, loss_fn, dataset_features_X, dataset_labels_Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
